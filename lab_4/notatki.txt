https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=6

1.używaj ReLu (ponieważ aktywacja następuje dla >0 można zainicjalizować małym biasem 0.0 - 0.1)
2.próbuj Leaky ReLu, Maxout, ELU
3.możesz spróbować tanh
4. NIE używaj sigmoidów (zabijają, niewydajne exp() )

Dla problemów dotyczących obrazów nie wykonuj nadmiernego preprocesingu!

Dobrym pomysłem jest wstępna inicjalizacja 'leyera'.
General Rule of thumb: Xavier initialization (zwana również Glorot initialization) jeżeli nie używasz ReLu (bo zabija połowę populacji)
Keras używa jej jako domyślnej więc należy ją zmodyfikować w celu zwiększenia wydajności sieci. Robi to na przykład
he et al initialization (zwana także jako Glorot Uniform)
https://medium.com/@prateekvishnu/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528

"You want unit gaussian activations? just make them so." ~ Sergey Ioffe, Christian Szegedy
Wg tej myśli możemy dodawać wartwy normalizujące batche.
Z reguły dodaje się je pomiędzy FC(fully conected) i (Convolutional) layers.


